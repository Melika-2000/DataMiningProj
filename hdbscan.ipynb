{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import DBSCAN,OPTICS,Birch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from functions import *\n",
    "import math\n",
    "import hdbscan\n",
    "plt.style.use(plt.style.available[5])\n",
    "\n",
    "\n",
    "# to ignore \"DtypeWarning\", generated due to reading csv files\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part 3\n",
    "\n",
    "products_DataSet = pd.read_csv('Data set/PRODUCTS.csv')\n",
    "\n",
    "productInstance_DataSet = pd.read_csv('Data set/PRODUCTINSTANCE.csv', encoding='cp1252')\n",
    "\n",
    "productInstance = productInstance_DataSet[['M_PRODUCT_ID',\"BOOKVALUE\"]].copy()\n",
    "products = products_DataSet[['M_PRODUCT_ID','NAME',\"VALUE\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************running*******************\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"*****************running*******************\")\n",
    "ls = []\n",
    "for i,record in enumerate(products['NAME']):\n",
    "    s1 = re.search(' . .',record)\n",
    "    s2 = re.search(' .',record)\n",
    "    if s1 :\n",
    "        if s1.end() == len(record) and s1.start() == len(record)-4:\n",
    "\n",
    "            if type(products.VALUE[i]) != type(1):\n",
    "                try:\n",
    "                    products.VALUE[i] = pd.to_numeric(products.VALUE[i]) \n",
    "                    products.VALUE[i] /= 100000000\n",
    "\n",
    "                except ValueError:\n",
    "                    products.VALUE[i] = products.VALUE[i][:len(products.VALUE[i])-8]\n",
    "\n",
    "\n",
    "    elif s2 :\n",
    "        if s2.end() == len(record) and s2.start() == len(record)-2:\n",
    "            if type(products.VALUE[i]) != type(1):\n",
    "                try:\n",
    "                    products.VALUE[i] = pd.to_numeric(products.VALUE[i]) \n",
    "                    products.VALUE[i] /= 10000\n",
    "\n",
    "                except ValueError:\n",
    "                    products.VALUE[i] = products.VALUE[i][:len(products.VALUE[i])-4]\n",
    "                    products.VALUE[i] = int(products.VALUE[i])\n",
    "\n",
    "\n",
    "    try:\n",
    "        products.VALUE[i] = pd.to_numeric(products.VALUE[i]) \n",
    "\n",
    "    except ValueError:\n",
    "        products.VALUE[i] = products.VALUE[i][:len(products.VALUE[i])-4]\n",
    "        products.VALUE[i] = pd.to_numeric(products.VALUE[i], errors='coerce') \n",
    "    \n",
    "\n",
    "products['VALUE'] = products['VALUE'].astype('float64')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "merged = pd.merge(left=products,right=productInstance,how='inner',left_on='M_PRODUCT_ID',right_on='M_PRODUCT_ID')\n",
    "\n",
    "\n",
    "cleanData = merged[['NAME',\"VALUE\",'BOOKVALUE']].dropna()\n",
    "\n",
    "Xc = cleanData[['VALUE','BOOKVALUE']].copy()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Xc.to_numpy()\n",
    "\n",
    "X_train_np, X_test_np = train_test_split(X,test_size=0.3,shuffle=True,random_state=40)\n",
    " # type: ignore\n",
    "# X_train_np = X_train.to_numpy()\n",
    "#  # type: ignore\n",
    "# X_test_np = X_test.to_numpy()\n",
    " # type: ignore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "standard = MinMaxScaler().fit(X)\n",
    "train = standard.transform(X_train_np)\n",
    "test = standard.transform(X_test_np)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated number of clusters: 86\n",
      "Estimated number of noise points: 242\n",
      "evaluating...\n",
      "Silhouette Coefficient: 0.666\n"
     ]
    }
   ],
   "source": [
    "from hdbscan import HDBSCAN\n",
    "\n",
    "# print('eps=0.00070,min_samples=10')\n",
    "# dbscan = DBSCAN(eps=0.000150,min_samples=3).fit(train)\n",
    "# dbscan = OPTICS(min_samples=10,max_eps=0.00070).fit(train)\n",
    "# dbscan = Birch(threshold=0.010,branching_factor=50).fit(train)\n",
    "model = HDBSCAN(min_cluster_size=5\n",
    ",cluster_selection_epsilon=0.000170\n",
    ",cluster_selection_method='eom'\n",
    ",prediction_data=True).fit(train)\n",
    "\n",
    "labels = model.labels_\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "print(\"Estimated number of clusters: %d\" % n_clusters_)\n",
    "print(\"Estimated number of noise points: %d\" % n_noise_)\n",
    "\n",
    "print('evaluating...')\n",
    "print(f\"Silhouette Coefficient: {metrics.silhouette_score(train, labels):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels: [1080 1512 1709   61  782 1710  407 2226 2701 1689]\n",
      "soft cluster scores: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "maxBoundary = len(test)\n",
    "index = []\n",
    "test_ls = []\n",
    "for i in range(10):\n",
    "    test_ls.append(test[random_int(0,maxBoundary)])\n",
    "\n",
    "test_ls = np.asarray(test_ls)\n",
    "test_labels, strengths = hdbscan.approximate_predict(model, test_ls)\n",
    "\n",
    "print(f\"labels: {test_labels}\")\n",
    "print(f\"soft cluster scores: {strengths}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "\n",
    "# figure = plt.figure(figsize=(18,18))\n",
    "\n",
    "# pal = sns.color_palette('deep', 110)\n",
    "# colors = [sns.desaturate(pal[col], sat) for col, sat in zip(model.labels_,\n",
    "#                                                             model.probabilities_)]\n",
    "# plt.scatter(train.T[0], train.T[1], c=colors)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d138ee4df375e66025d636822f618c5c0d3b0c70d03b3f5351c1d04343e1f86a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
